{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip uninstall httpx httpcore\n",
    "# !pip install --upgrade httpx httpcoreimport pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import builtins\n",
    "import csv\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:48:05.169927Z",
     "start_time": "2025-04-24T14:48:04.389776Z"
    }
   },
   "id": "bccb927f719ec96b",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "frequencydf should be a csv with columns: Language (e.g. 'Dutch'), 'POS' and for best results, 'Translation'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8526caf894cbb357"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0    Dutch  Frequency SpaCy_POS                      SpaCy_Tag  \\\n0              0       ik    7840843      PRON   VNW|pers|pron|nomin|vol|1|ev   \n1              1       je    7242582      PRON  VNW|pers|pron|nomin|red|2v|ev   \n2              2      het    5233420       DET              LID|bep|stan|evon   \n3              3       de    5171977       DET              LID|bep|stan|rest   \n4              4      dat    4505789     SCONJ                       VG|onder   \n...          ...      ...        ...       ...                            ...   \n3993        3993   gevolg       2399      NOUN      N|soort|ev|basis|onz|stan   \n3994        3994    troon       2399      NOUN     N|soort|ev|basis|zijd|stan   \n3995        3995      non       2399       SYM                       SPEC|afk   \n3996        3996  klappen       2398      NOUN               N|soort|mv|basis   \n3997        3997   zeuren       2396      VERB                   WW|pv|tgw|mv   \n\n              POS  Translation  \n0         Pronoun            I  \n1         Pronoun          you  \n2      Determiner           It  \n3      Determiner          the  \n4     Conjunction         that  \n...           ...          ...  \n3993         Noun  consequence  \n3994         Noun       throne  \n3995       Symbol          non  \n3996         Noun         clap  \n3997         Verb      to moan  \n\n[3998 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Dutch</th>\n      <th>Frequency</th>\n      <th>SpaCy_POS</th>\n      <th>SpaCy_Tag</th>\n      <th>POS</th>\n      <th>Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>ik</td>\n      <td>7840843</td>\n      <td>PRON</td>\n      <td>VNW|pers|pron|nomin|vol|1|ev</td>\n      <td>Pronoun</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>je</td>\n      <td>7242582</td>\n      <td>PRON</td>\n      <td>VNW|pers|pron|nomin|red|2v|ev</td>\n      <td>Pronoun</td>\n      <td>you</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>het</td>\n      <td>5233420</td>\n      <td>DET</td>\n      <td>LID|bep|stan|evon</td>\n      <td>Determiner</td>\n      <td>It</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>de</td>\n      <td>5171977</td>\n      <td>DET</td>\n      <td>LID|bep|stan|rest</td>\n      <td>Determiner</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>dat</td>\n      <td>4505789</td>\n      <td>SCONJ</td>\n      <td>VG|onder</td>\n      <td>Conjunction</td>\n      <td>that</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3993</th>\n      <td>3993</td>\n      <td>gevolg</td>\n      <td>2399</td>\n      <td>NOUN</td>\n      <td>N|soort|ev|basis|onz|stan</td>\n      <td>Noun</td>\n      <td>consequence</td>\n    </tr>\n    <tr>\n      <th>3994</th>\n      <td>3994</td>\n      <td>troon</td>\n      <td>2399</td>\n      <td>NOUN</td>\n      <td>N|soort|ev|basis|zijd|stan</td>\n      <td>Noun</td>\n      <td>throne</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>3995</td>\n      <td>non</td>\n      <td>2399</td>\n      <td>SYM</td>\n      <td>SPEC|afk</td>\n      <td>Symbol</td>\n      <td>non</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>3996</td>\n      <td>klappen</td>\n      <td>2398</td>\n      <td>NOUN</td>\n      <td>N|soort|mv|basis</td>\n      <td>Noun</td>\n      <td>clap</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>3997</td>\n      <td>zeuren</td>\n      <td>2396</td>\n      <td>VERB</td>\n      <td>WW|pv|tgw|mv</td>\n      <td>Verb</td>\n      <td>to moan</td>\n    </tr>\n  </tbody>\n</table>\n<p>3998 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = 'Dutch'\n",
    "language_code = 'nl' #For the translation API\n",
    "frequencydf = pd.read_csv(os.path.join('Frequency Lists', language, 'Clean.csv'))\n",
    "frequencydf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:48:42.923586Z",
     "start_time": "2025-04-24T14:48:42.882043Z"
    }
   },
   "id": "828a367aa491b845",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    open_api_key = config['open_api_key']\n",
    "client = OpenAI(api_key=open_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:48:49.203955Z",
     "start_time": "2025-04-24T14:48:49.171657Z"
    }
   },
   "id": "5fc9b97643525b85",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "SPLIT THE VOCABULARY INTO GROUPS OF STUDYSETS - default is 100 per set -> 1000, and 250 per set after."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c9d93cd2002f0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['Dutch', 'POS', 'Translation']\n",
      "remainder= 248\n"
     ]
    }
   ],
   "source": [
    "def split_into_studysets(frequencydf, language, initial_splitsize=100, new_splitsize=250):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Split_Sets'), exist_ok=True)\n",
    "    columns = [language]\n",
    "    if 'POS' in frequencydf.columns:\n",
    "        columns.append('POS')\n",
    "    if 'Translation' in frequencydf.columns:\n",
    "        columns.append('Translation')\n",
    "    if 'English' in frequencydf.columns:\n",
    "        frequencydf.rename(columns={'English': 'Translation'}, inplace=True)\n",
    "        columns.append('Translation')\n",
    "    if 'lemma' in frequencydf.columns:\n",
    "        columns.append('show')\n",
    "        columns.append('lemma')\n",
    "    print('columns:', columns)\n",
    "    \n",
    "    total_rows = len(frequencydf)\n",
    "    i = 0\n",
    "    splitsize = initial_splitsize\n",
    "    \n",
    "    while i * splitsize < total_rows:\n",
    "        if i * splitsize >= 1000:\n",
    "            splitsize = new_splitsize\n",
    "            start_index = 1000 + (i - 10) * new_splitsize  # Adjust start index for new splitsize\n",
    "        else:\n",
    "            start_index = i * splitsize\n",
    "        \n",
    "        end_index = min(start_index + splitsize, total_rows)\n",
    "        subdf = frequencydf[start_index:end_index][columns]\n",
    "        subdf.to_csv(language + '/Vocabulary/Split_Sets/' + str(start_index) + '-' + str(end_index) + '.csv')\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    remainder = total_rows % splitsize\n",
    "    print('remainder=', remainder)\n",
    "    if remainder > 0:\n",
    "        remaindersubdf = frequencydf[-remainder:][columns]\n",
    "        remaindersubdf.to_csv(language + '/Vocabulary/Split_Sets/' + 'Remainder.csv')\n",
    "\n",
    "    return subdf\n",
    "\n",
    "# Example usage\n",
    "df = split_into_studysets(frequencydf, language=language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:49:42.904810Z",
     "start_time": "2025-04-24T14:49:42.869377Z"
    }
   },
   "id": "58c4b9f973741e8e",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "OPTIONAL: If you only want to do a subset of files for time purposes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87b4b10f96a58987"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "selected_files = ['100-200.csv', '0-100.csv', '200-300.csv', '300-400.csv', '700-800.csv', '900-1000.csv', '500-600.csv', '400-500.csv', '600-700.csv', '800-900.csv']\n",
    "selected_files += ['1000-1250.csv', '1250-1500.csv','1500-1750.csv',\n",
    "'1750-2000.csv']\n",
    "# selected_files = ['2000-2250.csv','2250-2500.csv','2500-2750.csv',2750-3000.csv']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:56:41.390224Z",
     "start_time": "2025-04-24T14:56:41.386008Z"
    }
   },
   "id": "f1e2db1f88551b47",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def move_unselected_files(language, selected_files):\n",
    "    # Create directory for spare sets\n",
    "    spare_split_sets_dir = os.path.join(language, 'Vocabulary', 'Spare_Split_Sets')\n",
    "    os.makedirs(spare_split_sets_dir, exist_ok=True)\n",
    "\n",
    "    split_sets_dir = os.path.join(language, 'Vocabulary', 'Split_Sets')\n",
    "    all_files = os.listdir(split_sets_dir)\n",
    "    selected_files_set = set(selected_files)\n",
    "\n",
    "    # Move files not in selected_files to the spare sets directory\n",
    "    for file_name in all_files:\n",
    "        if file_name not in selected_files_set:\n",
    "            os.rename(\n",
    "                os.path.join(split_sets_dir, file_name),\n",
    "                os.path.join(spare_split_sets_dir, file_name)\n",
    "            )\n",
    "  # example of selected files\n",
    "move_unselected_files(language, selected_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:56:42.003465Z",
     "start_time": "2025-04-24T14:56:41.999326Z"
    }
   },
   "id": "8352a6d18e01aa0c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_sets_to_do(post_directory):\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    completed_sets = os.listdir(os.path.join(language, 'Vocabulary', post_directory))\n",
    "    allsets = os.listdir(os.path.join(language, 'Vocabulary', 'split_sets'))\n",
    "    incomplete_sets = [i for i in allsets if not i in completed_sets]\n",
    "    if '.DS_Store' in incomplete_sets:\n",
    "        incomplete_sets.remove('.DS_Store')\n",
    "    # print('Done:', completed_sets)\n",
    "    # print('To Do:', incomplete_sets)\n",
    "    return incomplete_sets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:56:43.347444Z",
     "start_time": "2025-04-24T14:56:43.339479Z"
    }
   },
   "id": "324822c4ae6b9b29",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT4 - Add example sentences for each word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5484fa0c59fef9b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100-200.csv']\n"
     ]
    }
   ],
   "source": [
    "CHATGPT_todosets = get_sets_to_do('ChatGPT_Sets')\n",
    "print(CHATGPT_todosets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T16:29:58.930293Z",
     "start_time": "2025-04-24T16:29:58.927987Z"
    }
   },
   "id": "c117c00eba8178c0",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_sentence_in_target_language(variable_word, POS, words_to_include, model = 'gpt-4o-2024-05-13', tenses = ['present', 'future', 'past']):\n",
    "    \"\"\"\n",
    "    Generate a sentence in Target Language using a specific word and part of speech, with words from a limited vocabulary, along with its English translation.\n",
    "\n",
    "    Parameters:\n",
    "    variable_word (str): The word to include in the sentence.\n",
    "    POS (str): The part of speech the word should operate as.\n",
    "    words_to_include (tuple): A list of words to try to include, and how many\n",
    "    model (str): The model to use for generating the sentence (default is 'gpt-4o-2024-05-13').\n",
    "    \n",
    "    # vocabulary (list): A list of words to use in the sentence.\n",
    "    # tenses (list)\n",
    "\n",
    "    Returns:\n",
    "    str: A sentence in Russian and its English translation, separated by a newline.\n",
    "    \"\"\"\n",
    "    \n",
    "    words_to_include = list(words_to_include[0].sample(words_to_include[1], replace=False))\n",
    "    # print(variable_word, POS)\n",
    "    # print('words_to_include:', words_to_include)\n",
    "    prompt = (\n",
    "              f\"Create a simple sentence in f{language} using basic vocabulary and containing the word '{variable_word}' \"\n",
    "              f\"operating as a {POS} part of speech. Also, provide the English translation of the sentence separated by a newline.\"\n",
    "              f\"Try to include the following words in the sentence: {words_to_include}\"\n",
    "              # f\"Use only the following tenses: {tenses}. \"\n",
    "              )\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        # Extract and return the output\n",
    "        output_message = response.choices[0].message.content.strip()\n",
    "        return output_message\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    # Extract and return the output\n",
    "    output_message = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return output_message"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:56:53.826409Z",
     "start_time": "2025-04-24T14:56:53.821586Z"
    }
   },
   "id": "6300157e037e763a",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iemand gaf hun een cadeau toen ze aankwamen.\n",
      "Someone gave them a gift when they arrived.\n",
      "De man komt naar huis, zei hij.\n",
      "The man comes home, he said.\n",
      "Ik kan niets komen helpen.  \n",
      "I can't come help with anything.\n",
      "De mensen luisterden de eerste keer toen hij zei.\n",
      "\n",
      "The people listened the first time when he said.\n"
     ]
    }
   ],
   "source": [
    "#TEST THE API\n",
    "for index, row in frequencydf.iloc[104:108].iterrows():\n",
    "    sentence = generate_sentence_in_target_language(row[language], row.POS, (frequencydf.iloc[100:200][language], 2))\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T14:57:01.213693Z",
     "start_time": "2025-04-24T14:56:55.937453Z"
    }
   },
   "id": "f46e7071d2767627",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_batch_sentences_in_target_language(batch_words, batch_pos, model='gpt-4o'):\n",
    "    \"\"\"Batch version that processes multiple words at once with smarter word inclusion.\"\"\"\n",
    "    # Use the batch itself as the word pool\n",
    "    words_to_include_pool = batch_words.copy()\n",
    "    \n",
    "    # Create list of word/POS tuples\n",
    "    words_with_pos = list(zip(batch_words, batch_pos))\n",
    "    \n",
    "    batch_prompt = f\"\"\"Create simple sentences in {language} using basic vocabulary for each of these words. \n",
    "    Each word should function as the specified part of speech.\n",
    "    \n",
    "    For each sentence, try to include 2 different additional words from this list: {', '.join(words_to_include_pool)}\n",
    "    \n",
    "    For each word, provide a simple {language} sentence and its English translation.\n",
    "    \n",
    "    Words: {\", \".join([f\"'{word}' ({pos})\" for word, pos in words_with_pos])}\n",
    "    \n",
    "    Provide response in JSON format:\n",
    "    {{\n",
    "      \"sentences\": [\n",
    "        {{\"word\": \"example\", \"dutch_sentence\": \"Dit is een korte voorbeeld zin.\", \"english_translation\": \"This is an example sentence.\", \"additional_words_used\": [\"korte\", \"voorbeeld\"]}},\n",
    "        ...\n",
    "      ]\n",
    "    }}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            max_tokens=60 * len(words_with_pos),\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        response_data = json.loads(response.choices[0].message.content)\n",
    "        results = {}\n",
    "        \n",
    "        for item in response_data[\"sentences\"]:\n",
    "            word = item[\"word\"]\n",
    "            sentence = f\"{item['dutch_sentence']}\\n{item['english_translation']}\"\n",
    "            results[word] = sentence\n",
    "        \n",
    "        # Ensure we return results in the same order as input\n",
    "        return [results.get(word, f\"Error for '{word}'\") for word in batch_words]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return [f\"An error occurred: {str(e)}\" for _ in batch_words]\n",
    "\n",
    "def Apply_Batch_LLM_to_studyset(s_set_df, set_name, batch_size=20):\n",
    "    \"\"\"Updated version of Apply_LLM_to_studyset with larger batches and progress tracking\"\"\"\n",
    "    # Process in batches with progress bar\n",
    "    results = []\n",
    "    batches = [s_set_df.iloc[i:i+batch_size] for i in range(0, len(s_set_df), batch_size)]\n",
    "    \n",
    "    for batch_df in tqdm(batches, desc=f\"Processing {set_name}\", unit=\"batch\"):\n",
    "        batch_words = batch_df[language].tolist()\n",
    "        batch_pos = batch_df['POS'].tolist()\n",
    "        \n",
    "        batch_results = generate_batch_sentences_in_target_language(\n",
    "            batch_words, batch_pos\n",
    "        )\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    # Add results back to dataframe\n",
    "    s_set_df['ChatGPT_Sentence'] = results\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T16:38:43.902564Z",
     "start_time": "2025-04-24T16:38:43.895689Z"
    }
   },
   "id": "75ea7b78e5156657",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T16:38:48.063190Z",
     "start_time": "2025-04-24T16:38:48.059832Z"
    }
   },
   "id": "5d4085d647ad7fab",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing set: 100-200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 100-200.csv: 100%|██████████| 5/5 [01:28<00:00, 17.73s/batch]\n"
     ]
    }
   ],
   "source": [
    "CHATGPT_todosets = get_sets_to_do('ChatGPT_Sets')\n",
    "for s_set in CHATGPT_todosets:\n",
    "    print(f\"\\nProcessing set: {s_set}\")\n",
    "    s_set_df = pd.read_csv(os.path.join(language, 'Vocabulary', 'split_sets', s_set))\n",
    "    Apply_Batch_LLM_to_studyset(s_set_df, s_set, batch_size=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-24T16:40:28.697237Z",
     "start_time": "2025-04-24T16:38:59.972281Z"
    }
   },
   "id": "249d3cce0318a7b4",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7eed2b79e298529"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def Apply_LLM_to_studyset(s_set_df, set_name, n = 2):\n",
    "    words_to_include = (s_set_df[language], n)\n",
    "    s_set_df['ChatGPT_Sentence'] = s_set_df.apply(lambda row: generate_sentence_in_target_language(row[language], row['POS'], words_to_include), axis = 1)\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary','ChatGPT_Sets', set_name))\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:47.932966Z",
     "start_time": "2024-07-15T17:29:47.927564Z"
    }
   },
   "id": "initial_id",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def do_all_sets(post_directory, func, language = language):\n",
    "    \"\"\" post-directory: directory where sets are stored\n",
    "    func: function to be applied to study_sets\"\"\"\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', post_directory), exist_ok = True)\n",
    "    for s_set in get_sets_to_do(post_directory):\n",
    "        if s_set[0] != '.': #IGNORE .DSStore\n",
    "            s_set_df = pd.read_csv(\n",
    "                os.path.join(language, 'Vocabulary', 'split_sets', s_set)\n",
    "            )\n",
    "            print(s_set)\n",
    "            func(s_set_df, s_set)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:29:49.294709Z",
     "start_time": "2024-07-15T17:29:49.276366Z"
    }
   },
   "id": "431870784f63c5e0",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['1000-1250.csv', '.DS_Store', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "do_all_sets('ChatGPT_Sets', Apply_LLM_to_studyset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:07.675552Z",
     "start_time": "2024-07-15T17:48:07.663211Z"
    }
   },
   "id": "4803e43262230313",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADD TRANSLATION INFORMATION TO SETS IF NECESSARY:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c95b5e763d556a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_set_translation_information(s_set_df, set_name, code = language_code):\n",
    "    LLM_Set = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    \n",
    "    # Check whether translation is already in the data\n",
    "    if 'Translation' in LLM_Set.columns:\n",
    "        if 'POS' in LLM_Set.columns:\n",
    "            s_set_df = LLM_Set[[language, 'Translation']]\n",
    "        \n",
    "    else:\n",
    "        s_set_df['Translation'] = s_set_df.apply(\n",
    "            lambda row: GoogleTranslator(source= code, target='en').translate(row[language]), axis=1\n",
    "        )\n",
    "\n",
    "    s_set_df.to_csv(os.path.join(language, 'Vocabulary/Translated Sets', set_name), index=False)\n",
    "    return s_set_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:09.192527Z",
     "start_time": "2024-07-15T17:48:09.186791Z"
    }
   },
   "id": "12e9fbf3edff6829",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n"
     ]
    }
   ],
   "source": [
    "translation_todosets = get_sets_to_do('Translated Sets')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.112673Z",
     "start_time": "2024-07-15T17:48:10.104318Z"
    }
   },
   "id": "7d085b8180f44d76",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000-1250.csv\n",
      "Done: ['800-900.csv', '100-200.csv', '600-700.csv', '700-800.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: ['1000-1250.csv', '1500-1750.csv', '1250-1500.csv', '1750-2000.csv']\n",
      "     Unnamed: 0          Dutch      POS Translation\n",
      "0          1000           eind     Noun         end\n",
      "1          1001          deden     Verb         did\n",
      "2          1002          mijne  Pronoun        mine\n",
      "3          1003           gooi     Noun       throw\n",
      "4          1004           gast     Noun       guest\n",
      "..          ...            ...      ...         ...\n",
      "245        1245        procent     Noun    per cent\n",
      "246        1246  neergeschoten     Verb        shot\n",
      "247        1247          spoor     Noun       track\n",
      "248        1248      kilometer     Noun  kilometers\n",
      "249        1249        bezorgd     Verb     Worried\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1000-1250.csv\n",
      "     Unnamed: 0       Dutch        POS   Translation\n",
      "0          1500       halve  Adjective          half\n",
      "1          1501  verdwijnen       Verb  to disappear\n",
      "2          1502     gelogen       Verb          lied\n",
      "3          1503        past       Verb         suits\n",
      "4          1504   bespreken       Verb       discuss\n",
      "..          ...         ...        ...           ...\n",
      "245        1745    verstand       Noun          mind\n",
      "246        1746      stuurt       Verb         sends\n",
      "247        1747       snapt       Verb   understands\n",
      "248        1748    oplossen       Verb      dissolve\n",
      "249        1749     vaarwel       Noun      farewell\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1500-1750.csv\n",
      "     Unnamed: 0      Dutch     POS       Translation\n",
      "0          1250   opdracht    Noun               Job\n",
      "1          1251   gevangen    Verb          captured\n",
      "2          1252       hoek    Noun            corner\n",
      "3          1253  vanmorgen  Adverb      this morning\n",
      "4          1254  verandert    Verb           changes\n",
      "..          ...        ...     ...               ...\n",
      "245        1495      opzij  Adverb             aside\n",
      "246        1496     openen    Verb           to open\n",
      "247        1497     tanden    Noun             teeth\n",
      "248        1498   hersenen    Noun             brain\n",
      "249        1499   getuigen    Verb  to give evidence\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1250-1500.csv\n",
      "     Unnamed: 0         Dutch          POS   Translation\n",
      "0          1750  ongelofelijk    Adjective  unbelievable\n",
      "1          1751         video         Noun         video\n",
      "2          1752          Adam  Proper noun          Adam\n",
      "3          1753        kregen         Verb           got\n",
      "4          1754        bevalt         Verb       pleases\n",
      "..          ...           ...          ...           ...\n",
      "245        1995          stof         Noun          dust\n",
      "246        1996           pot         Noun           pot\n",
      "247        1997        eieren         Noun          Eggs\n",
      "248        1998    verdedigen         Noun     to defend\n",
      "249        1999        kanker         Noun        cancer\n",
      "\n",
      "[250 rows x 4 columns]\n",
      "1750-2000.csv\n",
      "1500-1750.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1250-1500.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n",
      "1750-2000.csv\n",
      "Done: ['1000-1250.csv', '800-900.csv', '1500-1750.csv', '100-200.csv', '600-700.csv', '1250-1500.csv', '700-800.csv', '1750-2000.csv', '0-100.csv', '300-400.csv', '900-1000.csv', '400-500.csv', '500-600.csv', '200-300.csv']\n",
      "To Do: []\n"
     ]
    }
   ],
   "source": [
    "for set in translation_todosets:\n",
    "    print(set)\n",
    "    do_all_sets('Translated Sets', add_set_translation_information)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:48:10.595071Z",
     "start_time": "2024-07-15T17:48:10.560433Z"
    }
   },
   "id": "25893c8ba5ba6b07",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make into Quizlet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78468a35bd76d7aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_quizlet_ready_set(set_name):\n",
    "    #For the quizlet sets, the rows are separated by 3 blank lines \n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "    # For French - Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True)\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.strip()\n",
    "    \n",
    "    df['quizlet'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "    df = df[[language, 'quizlet']]\n",
    "    \n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row[language]}\\t{row['quizlet']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:13.426196Z",
     "start_time": "2024-07-15T17:52:13.417594Z"
    }
   },
   "id": "6f5dba1a4a2efcc8",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_English2Language_quizlet_set(set_name):\n",
    "    df = pd.read_csv(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets', set_name))\n",
    "    transdf = pd.read_csv(os.path.join(language, 'Vocabulary', 'Translated Sets', set_name))\n",
    "\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "    # Removing text within parentheses that start with 'see'\n",
    "    df['ChatGPT_Sentence'] = df['ChatGPT_Sentence'].str.replace(r'\\(see[^)]*\\)', '', regex=True).str.strip()\n",
    "    \n",
    "    df['quizlet_front'] = df['POS'] + ' : ' + '*' + transdf['Translation'] + '*'\n",
    "    df['quizlet_back'] =  '*' + df[language] + '*' + '\\n' + df['ChatGPT_Sentence']\n",
    "\n",
    "    df = df[['quizlet_front', 'quizlet_back']]\n",
    "    os.makedirs(os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets'), exist_ok=True)\n",
    "    output_path = os.path.join(language, 'Vocabulary', 'Reverse Quizlet Sets', set_name.replace('.csv', '.txt'))\n",
    "\n",
    "    # Saving the data to a tab-separated text file with 3 lines between each row\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write(f\"{row['quizlet_front']}\\t{row['quizlet_back']}\\n\\n\\n\")\n",
    "\n",
    "    print(f\"Text file saved to {output_path}\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:14.312762Z",
     "start_time": "2024-07-15T17:52:14.300455Z"
    }
   },
   "id": "61730a4d0b6bb5b3",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'0-100.csv',\n '100-200.csv',\n '1000-1250.csv',\n '1250-1500.csv',\n '1500-1750.csv',\n '1750-2000.csv',\n '200-300.csv',\n '300-400.csv',\n '400-500.csv',\n '500-600.csv',\n '600-700.csv',\n '700-800.csv',\n '800-900.csv',\n '900-1000.csv'}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# builtin_set = builtins.set\n",
    "LLM_sets = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'ChatGPT_Sets')))\n",
    "Translated_Sets = builtin_set(os.listdir(os.path.join(language, 'Vocabulary', 'Translated Sets')))\n",
    "quizlet_todosets = reverse_quizlet_todosets = LLM_sets.intersection(Translated_Sets)\n",
    "\n",
    "for s_set in quizlet_todosets:\n",
    "    make_quizlet_ready_set(s_set)\n",
    "    make_English2Language_quizlet_set(s_set)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T17:52:15.559954Z",
     "start_time": "2024-07-15T17:52:15.553871Z"
    }
   },
   "id": "f317b89326596dd2",
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
